{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Parallel Uploads\n",
    "\n",
    "`This notebook is a Python2 notebook`\n",
    "\n",
    "There are few examples of uploading millions of small files from a local (or cloud based) compute instance with attached block storage up into S3. \n",
    "\n",
    "Frequently, files are compressed/zipped into a single file then uploaded to S3 and are then pulled down and unzipped at training time. \n",
    "\n",
    "However, in the case of image based training data, particularly where the total dataset size is larger than the memory of a single instance, having the ability to add additional images to storage w/o having to download/unzip/add/rezip/upload is essential to a startup wanting to manage their storage in a Cloud Object Storage bucket. \n",
    "\n",
    "I decided to run some tests using 8,000,000 MNIST images in sub-optimal file folder configuration (10 sub-folders for numbers zero to nine). I ran my tests on an AWS ml.m5.24xlarge instance. That features 96 cores, 384Gb Ram, and 25 Gigabit connection... basically directly into S3. \n",
    "\n",
    "The goal is simple: upload the files as fast as possible. \n",
    "\n",
    "In testing, the AWS CLI \"s3 sync\" and \"s3 copy\" menthods produce unsatisfactorily poor performance in these high quantity small file type uploads. My first test runs ran 4+ hours before I terminated them and looked for alternative methods. They also faced limitations on their ability to scale & effectively utilze multiple cores & saturate the bandwith of my testing instance. \n",
    "\n",
    "Attempts to improve the performance prior to abandoning the CLI included [modifying the configuration to increase the number of threads & cache size](https://aws.amazon.com/premiumsupport/knowledge-center/s3-improve-transfer-sync-command/), partitioning the data and running multiple instances of the AWS CLI shell commands using [filters](https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#use-of-exclude-and-include-filters) for parallel sync, and partitioning by subfolder (prefix) w/ parllel sync commands. \n",
    "\n",
    "In my research I tested a few other options including writing my own script using the Boto3. \n",
    "The best performing tools I found were [rclone](https://rclone.org/) & [s3-parallel-put](https://github.com/mishudark/s3-parallel-put)\n",
    "\n",
    "Here is how I recommend to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rclone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4437  100  4437    0     0  13865      0 --:--:-- --:--:-- --:--:-- 13822\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0     47      0 --:--:-- --:--:-- --:--:--    47\n",
      "\n",
      "The latest version of rclone rclone v1.51.0 is already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install\n",
    "!curl https://rclone.org/install.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/04/23 19:07:42 NOTICE: Config file \"/home/ec2-user/.config/rclone/rclone.conf\" not found - using defaults\r\n",
      "Remote config\r\n",
      "--------------------\r\n",
      "[s3]\r\n",
      "type = s3\r\n",
      "provider = AWS\r\n",
      "env_auth = true\r\n",
      "region = us-east-1\r\n",
      "acl = private\r\n",
      "--------------------\r\n"
     ]
    }
   ],
   "source": [
    "# Make an rclone config for s3. (Can also be done for GCP. Run `rclone config create` manually in terminal)\n",
    "!rclone config create s3 s3 provider AWS env_auth true region us-east-1 acl private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rclone.org/docs/\n",
    "# https://rclone.org/s3/#amazon-s3\n",
    "\n",
    "# Monitor w/ htop (networking) and tail -f \n",
    "# Run rclone without TPS rate limiting, but use massive error handling.\n",
    "# Limit the number of \"retries\" to 3. \n",
    "# Transfers = 10x number of cores. 10 threads per core. \n",
    "# Max back log = # of files if you have enough ram to handle it. \n",
    "!rclone copy /home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker-python-sdk/pytorch_horovod_mnist/mnist8m_img s3:sagemaker-scratch-1234o2ijwoer23423/sagemaker/pytorch-mnist8m_rclone_copyftn --stats-one-line-date --transfers=960 --max-backlog=82000000 --low-level-retries=5000 --fast-list --local-no-check-updated --retries=2 --log-file=rclone_log_file_copy --stats-log-level=NOTICE\n",
    "\n",
    "# Rclone may log some errors if it is overwhelming s3 (> 3500 puts/s on a single prefix.)\n",
    "# I recommend using `!tail -f /logfile` to observe the log outputs in real time. \n",
    "# I also used iftop & htop to monitor the CPU utilization & Network utilization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/04/23 23:20:28 NOTICE: 2020/04/23 23:20:28 -    56.277M / 2.493 GBytes, 2%, 1.069 MBytes/s, ETA 38m54s (xfr#167438/8109993)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Examining the Log Head\n",
    "!head -2 /home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker-python-sdk/pytorch_horovod_mnist/rclone_log_file_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/04/24 00:13:28 NOTICE: 2020/04/24 00:13:28 -     2.615G / 2.615 GBytes, 100%, 932.323 kBytes/s, ETA 0s\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Examining the Log Tail\n",
    "!tail -2 /home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker-python-sdk/pytorch_horovod_mnist/rclone_log_file_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 3180 seconds.\n",
      "Or you could say it took 53 minutes.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "seconds = (datetime.strptime('2020/04/24 00:13:28', '%Y/%m/%d %H:%M:%S') - datetime.strptime('2020/04/23 23:20:28', '%Y/%m/%d %H:%M:%S')).seconds\n",
    "print (\"It took \" + str(seconds) + \" seconds.\" )\n",
    "print (\"Or you could say it took \" + str(seconds/60) + \" minutes.\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average upload speed was around 822.327044025 kBytes/second\n"
     ]
    }
   ],
   "source": [
    "speed = 2.615/seconds\n",
    "print (\"Average upload speed was around \"+ str(speed*1000*1000) + \" kBytes/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use [s3-parallel-put](https://github.com/mishudark/s3-parallel-put)\n",
    "\n",
    "This S3-Parallel-Put library works pretty well, but it is a Python2.x lib. \n",
    "It walks the filesystem, creates a queue, puts to S3, logs the result. Simple & Easy to use. \n",
    "\n",
    "It is fast because it uses multiprocessing on the backend. \n",
    "My only concern with it is if it would resume gracefully if it was to fail during a large upload. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/python2/lib/python2.7/site-packages (2.48.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting python-magic\n",
      "  Downloading https://files.pythonhosted.org/packages/42/a1/76d30c79992e3750dac6790ce16f056f870d368ba142f83f75f694d93001/python_magic-0.4.15-py2.py3-none-any.whl\n",
      "Installing collected packages: python-magic\n",
      "Successfully installed python-magic-0.4.15\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "--2020-04-30 00:34:27--  https://raw.githubusercontent.com/mishudark/s3-parallel-put/master/s3-parallel-put\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19600 (19K) [text/plain]\n",
      "Saving to: ‘/usr/bin/s3-parallel-put’\n",
      "\n",
      "/usr/bin/s3-paralle 100%[===================>]  19.14K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2020-04-30 00:34:28 (24.8 MB/s) - ‘/usr/bin/s3-parallel-put’ saved [19600/19600]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install boto\n",
    "!pip install python-magic\n",
    "!sudo wget -O /usr/bin/s3-parallel-put https://raw.githubusercontent.com/mishudark/s3-parallel-put/master/s3-parallel-put\n",
    "!sudo chmod +x /usr/bin/s3-parallel-put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 11.5 s, total: 1min 56s\n",
      "Wall time: 1h 36min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://github.com/mishudark/s3-parallel-put\n",
    "# Wall time: 1hr 36min 23s\n",
    "!s3-parallel-put --bucket=sagemaker-scratch-1234o2ijwoer23423 --prefix=sagemaker/pytorch-mnist8m-s3-parallel-upload --walk=filesystem --put=stupid --log-filename=s3-upload-log --insecure --processes=96 /home/ec2-user/SageMaker/amazon-sagemaker-examples/sagemaker-python-sdk/pytorch_horovod_mnist/mnist8m_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:s3-parallel-put[statter-20506]:put 2674526724 bytes in 8110001 files in 5783.4 seconds (462452 bytes/s, 1402.3 files/s)\r\n"
     ]
    }
   ],
   "source": [
    "!tail -1 s3-upload-log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put speed was 462 kBytes/Second\n"
     ]
    }
   ],
   "source": [
    "kBytesPerSec = 462452/1000\n",
    "print (\"Put speed was \"+ str(kBytesPerSec) + \" kBytes/Second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
